{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Regression\n",
    "\n",
    "1. **What is linear regression ?**\n",
    "   - Algorithm that use a **linear combination of features to predict a continuous target**\n",
    "   - **Weights are multiplied to each of the features** prior to linear combination\n",
    "   - Weights are adjusted based on the error of the linear combination with the actual target value\n",
    "\n",
    "   <br>\n",
    "\n",
    "2. **What are the assumptions of a linear regression model ?**\n",
    "   - **Constant variance** of feature across different target values (homoscedasticity)\n",
    "   - **Normal distribution** of residuals of the model\n",
    "   - **Independence** between data points (lack of autocorrelation)\n",
    "   - **Linearity** of linear combination of features and target\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. **What are the methods to test the assumptions ?**\n",
    "   - **Heteroscedasticity** if residual plot presents a fanning pattern\n",
    "   - **Non-normal** if the quantiles are not aligned in the Q-Q plot with the normal quantiles\n",
    "   - **Dependence or Non-linearity** if the clear trends are observed in a sequence of points in residual plot\n",
    "\n",
    "   <br>\n",
    "   \n",
    "4. **What are the steps one can take to correct for the violated assumptions ?**\n",
    "   - **Heteroscedasticity** can be reduced by taking log of the features / target variables\n",
    "   - **Non-normality** of residuals can be reduced by taking log of the features / target variables\n",
    "   - **Dependence** can be taken into account by including lag terms or more complex time series model\n",
    "   - **Non-linearity** can be taken into account by including square or interaction term\n",
    "   \n",
    "   <br>\n",
    "\n",
    "5. **What is multicollinearity, why is a problem ?**\n",
    "   \n",
    "   - Multicollinearity is when **two or more features are linearly correlated with each other**\n",
    "   - The **coefficients** of the regression model become **unstable** and do not reflect the true coefficients\n",
    "   - The influence of collinear features on the target is **randomly assigned to the coefficients of those features**\n",
    "   - Removing or **adding a data point** might **change coefficients** significantly\n",
    "   \n",
    "   <br>\n",
    "   \n",
    "6. **How do we detect multicollinearity ? Under what circumstances can we overlook multicollinearity ?**\n",
    "\n",
    "   - Compute the **Variance Inflation Factor (VIF), only for linear regression**\n",
    "     - Loop through each feature and run a regression model using each feature as the target\n",
    "     - The VIF of the particular feature $i$ is $\\frac{1}{1-R_i^2}$\n",
    "   - **VIF > 10 in general is regarded as a sign of collinearity**\n",
    "   - We can **overlook multicollinearity** if we are only **concerned with predictions**\n",
    "   - But the test data must have same collinearity structure as the trained data\n",
    "\n",
    "   <br>\n",
    "   \n",
    "7. **What is the interpretation of $\\beta$ ? What does $\\beta_0$ (the bias) mean ?**\n",
    "\n",
    "   - A unit of **increase of a certain feature** is correlated with **$\\beta$ amount of increase / decrease in the target** value holding all the other features constant\n",
    "   - $\\beta_0$ represents the **mean value of target when all the other features have a value of zero**\n",
    "\n",
    "   <br>\n",
    "\n",
    "8. **How does the interpretation of $\\beta$ change if log is taken on the the features / target variables ?**\n",
    "   - **Logging the target:** \n",
    "   \n",
    "     A unit of increase of a feature correlated with (100 x $\\beta$) % increase / decrease in the target\n",
    "     \n",
    "     <br>\n",
    "     \n",
    "   - **Logging a feature:** \n",
    "   \n",
    "     1 % of increase of a feature correlated with ($\\beta$ / 100) units increase / decrease in the target\n",
    "   \n",
    "     <br>\n",
    "\n",
    "   - **Logging a feature and the target:** \n",
    "   \n",
    "     1 % of increase of a feature correlated with $\\beta$ % increase / decrease in the target\n",
    "   \n",
    "     <br>\n",
    "\n",
    "9. **What is the interpretation of the p-value of $\\beta$ ? What is the null and alternative hypothesis ?**\n",
    "\n",
    "   - **Null:** $\\beta =$ 0 \n",
    "   - **Alternative:** $\\beta \\neq$  0\n",
    "   - If the p-value is less than the significance level, then we can reject the null of $\\beta =$ 0, i.e. feature is not correlated to changes in the target variable\n",
    "\n",
    "   <br>\n",
    "\n",
    "10. **Can you compare beta coefficient in multiple linear regression ? If not, what will allow us to ?**\n",
    "\n",
    "   - **Normalizing** (substract by mean and divided by standard deviation column-wise) **the features and target**\n",
    "   - Since variables are normalized, we do not need to fit a bias ($\\beta_0$) term\n",
    "\n",
    "   <br>\n",
    "\n",
    "11. **What are some causes for under-fitting ? How to avoid them ? What disadvantages do they bring ?**\n",
    "\n",
    "   - **Causes:**\n",
    "     - Overly simplistic model, not enough feature\n",
    "     - Not enough polynomial / transformation of features\n",
    " \n",
    "   - **Consequences:**\n",
    "     - High bias, low variance\n",
    "     - Inaccurate predictions due to under-learning from the data\n",
    "   \n",
    "   - **Solution:**\n",
    "     - More features and complex (non-linear) model\n",
    "\n",
    "   <br>\n",
    "   \n",
    "12. **What are some causes for over-fitting ? How to avoid them ?**\n",
    "\n",
    "   - **Causes:**\n",
    "     - Overly complex model, too many feature\n",
    "     - Too many polynomial / transformation of features\n",
    " \n",
    "   - **Consequences:**\n",
    "     - Low bias, high variance\n",
    "     - Inaccurate predictions due to over-learning from noise and edge cases in the data\n",
    "   \n",
    "   - **Solution:**\n",
    "     - Regularization\n",
    "     - Feature selection\n",
    "     - Use K-fold cross validation to decide complexity of model\n",
    "     \n",
    "   <br>\n",
    "\n",
    "13. **How to spot an outlier in multiple linear regression ?**\n",
    "\n",
    "   - **Univariate scatter plots / Boxplots**\n",
    "   - **Normalized residual plot:**\n",
    "     -  More than 2 standard deviation from mean considered as outlier\n",
    "   - High residual values in additional to high leverage makes an influential outlier\n",
    "\n",
    "   <br>\n",
    "   \n",
    "14. **What does it mean in multiple regression when a point has high levarage ? What constitue an influential point ? Why are they important ?**\n",
    "\n",
    "   - **High levarage** means some feature(s) of a data point far deviates from the mean value of the feature(s) across data point\n",
    "   - **Influence = Leverage x Residual**\n",
    "   - High leverage and high residual data points are influential, i.e. affects the model ($\\beta$s) a lot when absent / present\n",
    "   - **Important because you do not a few data points controlling your model, and if so, you should know why**\n",
    "    \n",
    "    <br>\n",
    "\n",
    "15. **How to determine how many features to select for your model ? What difference does it make if you are fitting on on a large (does not fit into memory) dataset ?**\n",
    "    \n",
    "   - **Forwards / Backwards elimination** (Not feasible with big dataset, computationally intensive)\n",
    "   - **Lasso** (L1) regularization (shrink unimportant features towards zero)\n",
    "   - **Random forest feature importance**\n",
    "   - **Adjusted $R^2$ with K-fold cross validation** (F1 score with K-fold if logistic regression)\n",
    "   - Product knowledge\n",
    "   - Univariate correlation / chi-square (if categorical) with target\n",
    "\n",
    "    <br>\n",
    "  \n",
    "16. **What is the cost function for linear regression ?**\n",
    "\n",
    "   - **Mean Squared Error (MSE)**, $\\sum_{i=1}^n (\\hat{y} - y)^2$\n",
    "\n",
    "    <br>\n",
    "    \n",
    "17. **Using the cost function, describe two algorithms to fit a linear regression model.**\n",
    "  \n",
    "   **Analytical Approach**\n",
    "   - Take the partial derivate with respect to the particular $\\beta$ of the cost function. That is the gradient of the cost with respect to the particular $\\beta$\n",
    "     \n",
    "     $$\\frac{\\partial}{\\partial{\\beta_i}} \\sum_{d=1}^n (\\beta^T X - y)^2$$\n",
    "     \n",
    "   - Set the gradient to 0 and solve for $\\beta$\n",
    "   \n",
    "   <br>\n",
    "     \n",
    "   **Gradient Descent**\n",
    "   - If regularization / transformation / polynomial terms are involved (derivate not closed-form or hard to solve)\n",
    "   - Randomly initialize parameters and compute **gradient (as shown above)** and **cost**\n",
    "   - Add / Minus (depending on cost function) product of learning rate and gradient\n",
    "   - Repeat until cost is minimized / maximized (depending on cost function) \n",
    "   - Usually use second order method (gradient of gradient) or Hessian free ([LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)) as it is more efficient\n",
    "\n",
    "   <br>\n",
    "  \n",
    "18. **What are the differences between running linear regression on a large (does not fit into memory) vs a small dataset ?**\n",
    "\n",
    "   **Optimization**\n",
    "\n",
    "   - Cannot do optimization on the whole dataset\n",
    "   - Stochastic gradient descent / Mini-batch\n",
    "   - Compute gradient and update parameters $\\beta$s on one / a few data points\n",
    "   - Until the convergence of cost\n",
    "   \n",
    "   <br>\n",
    "   \n",
    "   **Interpretation**\n",
    "   \n",
    "   - p-value of $\\beta$ are not meaningful any more. Will be statistically significant given large enough data\n",
    "   - Normality assumption is usually met (due to large dataset)\n",
    "   - Usually cannot examine residual plot for diagnostics (unless sample data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##Logistic Regression\n",
    "\n",
    "**Questions for Linear Regression are applicable to Logistic Regression, except for the ones listed below.**  \n",
    "\n",
    "<br>\n",
    "\n",
    "1. **What is the difference between logistic regression and linear regression ?**\n",
    "\n",
    "   - Logistic regression employs the logit function to model a **discrete non-continuous and binary target**\n",
    "   - Logistic regression models the **probability of obtaining a positive class** (`y=1`) \n",
    "   - A **linear relationship would not suffice in modeling the probability** since slight changes in the values of the features would change the probability of obtaining a positive class when the probability is close to the decision boundary between the positive and negative class\n",
    "   - $p(\\text{Y=1 | X; }\\theta) = \\frac{1}{1 + e^{-\\theta^TX}}$\n",
    "   \n",
    "   <br>\n",
    "\n",
    "2. **How do you use logistic regression to tackle a classification problem with multiple labels ?**\n",
    "\n",
    "   - **Multiple one-vs-rest logistic regression model (Softmax Regression)**\n",
    "   - $p(\\text{Y=i | X; }\\theta) = \\frac{e^{\\theta_i^TX}}{\\sum_{j=1}^k e^{-\\theta_j^TX}}$\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. **Interpretation of logistic regression model coefficients**\n",
    "\n",
    "   - Say $\\beta_1 = -0.0621$, then a unit of increase in feature 1 would lead to $e^{-0.0621} = 0.94$ as likely (in odds) to observe target = 1\n",
    "   - Say $\\beta_1 = 0.0621$, then a unit of increase in feature 1 would lead to $e^{0.0621} = 1.06$ as likely (in odds) to observe target = 1\n",
    "   - Say $\\beta_1 = 0.0621$ and feature 1 is $log_2$ transformed, then doubling feature 1 would lead to $e^{0.0621} = 1.06$ as likely to observe target = 1\n",
    "\n",
    "   <br>\n",
    "   \n",
    "4. **How to find the decision boundary in the feature space for a trained logistic regression model ? (Work through an example when regressed with 1 feature)**\n",
    "\n",
    "   - **Say $p=0.5$ for the boundary between positive and negative classes**, and we have $\\theta$ since the model is trained \n",
    "   \n",
    "     $$0.5 = \\frac{1}{1 + e^{-\\theta^TX}}$$\n",
    "     \n",
    "     <br>\n",
    "     \n",
    "   - Rearrange the equation above to get X on the left hand side and plot the line\n",
    "   \n",
    "   <br>\n",
    "\n",
    "5. **What is the cost function for logistic regression ?**\n",
    "\n",
    "   - **Likelihood:** \n",
    "   \n",
    "      $$\\prod_{i=1}^n logit(\\theta^TX)^y \\times (1 - logit(\\theta^TX))^{(1-y)}$$\n",
    "      \n",
    "      <br>\n",
    "      \n",
    "   - **Log Likelihood:** \n",
    "   \n",
    "      $$\\sum_{i=1}^n y \\cdot log(logit(\\theta^TX)) + (1-y) \\cdot log(1 - logit(\\theta^TX))$$\n",
    "      \n",
    "      <br>\n",
    "      \n",
    "   - **Negative Log Likelihood :**\n",
    "   \n",
    "      $$- \\sum_{i=1}^n y \\cdot log(logit(\\theta^TX)) + (1-y) \\cdot log(1 - logit(\\theta^TX))$$\n",
    "\n",
    "   <br>\n",
    "   \n",
    "6. **Coefficient of features (in logistic regression) is the opposite sign of what you expect. What might be the problem ?**\n",
    "\n",
    "   **Check for:**\n",
    "   - Data validity \n",
    "   - Outliers \n",
    "   - Multicollinearity (Solution: PCA, regularization)\n",
    "   - Confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Regularization\n",
    "\n",
    "1. **Why is regularization necessary ?**\n",
    "\n",
    "   - When there is high variance in a model (over-fitting)\n",
    "   - Make the model fit the noise in the data less and the underlying phenomenon / signal more\n",
    "\n",
    "   <br>\n",
    "   \n",
    "2. **What are some ways to regularize a logistic regression model ?**\n",
    "\n",
    "   - Lasso (L1) \n",
    "   - Ridge (L2)\n",
    "   - PCA\n",
    "   - Select fewer features / Better feature selection (See Q15 above)\n",
    "\n",
    "   <br>\n",
    "   \n",
    "3. **Briefly describe the principles behind (Lasso and Ridge) regularization ? What are the differences between Lasso and Ridge regularization ?**\n",
    "   \n",
    "   - Lasso / Ridge regularization can be thought of as a kind of Bayesian estimate of the beta coefficients\n",
    "   - Lasso (L1) assumes the coefficients are distributed in a Lapalacian distribution\n",
    "   - Ridge (L2) assumes the coefficients are distributed in a Gaussian distribution\n",
    "   - As seen in the below illustration, Lapalacian favors values at zero and large values and Gaussian favors small values\n",
    "     ![](images/linear_logistic_regression/l1_l2.png)\n",
    "   - Given two features that highly correlated, L1 will assign a zero coefficient to one feature and a large coefficient to another, L2 will assign small coefficients to both features\n",
    "   - L1 performs feature selection resulting in a model with fewer features, but L2 often performs better in prediction\n",
    "   - Elastic Net (a combination of L1 and L2) is (in theory) better than L1 / L2 alone\n",
    "   - **Cost functions for L1 and L2:**\n",
    "     - **L1:** \n",
    "     \n",
    "       $$\\text{Cost Function (linear / logistic)} + \\lambda \\sum_{i=1}^{n-1} |\\beta_i|$$\n",
    "       \n",
    "       <br>\n",
    "       \n",
    "     - **L2:**\n",
    "     \n",
    "       $$\\text{Cost Function (linear / logistic)} + \\lambda \\sum_{i=1}^{n-1} \\beta_i^2$$\n",
    "       \n",
    "     - $\\lambda$ should be selected based on prediction performance on K-fold cross validation\n",
    "\n",
    "   - [Reference 1](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization), \n",
    "     [Reference 2](http://statweb.stanford.edu/~jtaylo/courses/stats203/notes/penalized.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "1. **Explain what precision, recall and specificity are.**\n",
    "\n",
    "   <br>\n",
    "\n",
    "2. **What are the differences between a precision-recall curve and a ROC curve ?**\n",
    "\n",
    "   <br>\n",
    "   \n",
    "3. **You have built a logistic regression model to predict fraud, how do you decide the threshold for deciding fraud or not fraud ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Validation\n",
    "\n",
    "1. **How do you test a model to ensure it is robust ?**\n",
    "\n",
    "   <br>\n",
    "\n",
    "2. **Explain K-fold cross validation.** \n",
    "\n",
    "   <br>\n",
    "\n",
    "3. **Given you have trained and tested a model with cross validation and you deploy the model and it performs poorly. Provide some reasonable explanations.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
