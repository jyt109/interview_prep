{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Regression\n",
    "\n",
    "1. **What is linear regression ?**\n",
    "   - Algorithm that use a linear combination of features to predict a continuous target\n",
    "   - Weights are multiplied to each of the features prior to linear combination\n",
    "   - Weights are adjusted based on the error of the linear combination with the actual target value\n",
    "\n",
    "   <br>\n",
    "\n",
    "2. **What are the assumptions of a linear regression model ?**\n",
    "   - **Constant variance** of feature across different target values (homoscedasticity)\n",
    "   - **Normal distribution** of residuals of the model\n",
    "   - **Independence** between data points (lack of autocorrelation)\n",
    "   - **Linearity** of linear combination of features and target\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. **What are the methods to test the assumptions ?**\n",
    "   - **Heteroscedasticity** if residual plot presents a fanning pattern\n",
    "   - **Non-normal** if the quantiles are not aligned in the Q-Q plot with the normal quantiles\n",
    "   - **Dependence or Non-linearity** if the clear trends are observed in a sequence of points in residual plot\n",
    "\n",
    "   <br>\n",
    "   \n",
    "4. **What are the steps one can take to correct for the violated assumptions ?**\n",
    "   - **Heteroscedasticity** can be reduced by taking log of the features / target variables\n",
    "   - **Non-normality** of residuals can be reduced by taking log of the features / target variables\n",
    "   - **Dependence** can be taken into account by including lag terms or more complex time series model\n",
    "   - **Non-linearity** can be taken into account by including square or interaction term\n",
    "   \n",
    "   <br>\n",
    "\n",
    "5. **What is multicollinearity, why is a problem ?**\n",
    "   \n",
    "   - Multicollinearity is when **two or more features are linearly correlated with each other**\n",
    "   - The **coefficients** of the regression model become **unstable** and do not reflect the true coefficients\n",
    "   - The influence of collinear features on the target is **randomly assigned to the coefficients of those features**\n",
    "   - Removing or **adding a data point** might **change coefficients** significantly\n",
    "   \n",
    "   <br>\n",
    "   \n",
    "6. **How do we detect multicollinearity ? Under what circumstances can we overlook multicollinearity ?**\n",
    "\n",
    "   - Compute the **Variance Inflation Factor (VIF), only for linear regression**\n",
    "     - Loop through each feature and run a regression model using each feature as the target\n",
    "     - The VIF of the particular feature $i$ is $\\frac{1}{1-R_i^2}$\n",
    "   - **VIF > 10 in general is regarded as a sign of collinearity**\n",
    "   - We can overlook multicollinearity if we are only concerned about get accurate predictions\n",
    "   - But the test data must have same collinearity structure as the trained data\n",
    "\n",
    "   <br>\n",
    "   \n",
    "7. **What is the interpretation of $\\beta$ ? What does $\\beta_0$ (the bias) mean ?**\n",
    "\n",
    "   - A unit of increase of a certain feature is correlated with $\\beta$ amount of increase / decrease in the target value holding all the other features constant\n",
    "   - $\\beta_0$ represents the mean value of target when all the other features have a value of zero\n",
    "\n",
    "   <br>\n",
    "\n",
    "8. **How does the interpretation of $\\beta$ change if log is taken on the the features / target variables ?**\n",
    "   - **Logging the target:** \n",
    "   \n",
    "     A unit of increase of a feature correlated with (100 x $\\beta$) % increase / decrease in the target\n",
    "     \n",
    "     <br>\n",
    "     \n",
    "   - **Logging a feature:** \n",
    "   \n",
    "     1 % of increase of a feature correlated with ($\\beta$ / 100) units increase / decrease in the target\n",
    "   \n",
    "     <br>\n",
    "\n",
    "   - **Logging a feature and the target:** \n",
    "   \n",
    "     1 % of increase of a feature correlated with $\\beta$ % increase / decrease in the target\n",
    "   \n",
    "     <br>\n",
    "\n",
    "9. **What is the interpretation of the p-value of $\\beta$ ? What is the null and alternative hypothesis ?**\n",
    "\n",
    "   - **Null:** $\\beta =$ 0 \n",
    "   - **Alternative:** $\\beta \\neq$  0\n",
    "   - If the p-value is less than the significance level, then we can reject the null of $\\beta =$ 0, i.e. feature is not correlated to changes in the target variable\n",
    "\n",
    "   <br>\n",
    "\n",
    "10. **Can you compare beta coefficient in multiple linear regression ? If not, what will allow us to ?**\n",
    "\n",
    "   - Normalizing (substract by mean and divided by standard deviation column-wise) the features and target\n",
    "   - Since variables are normalized, we do not need to fit a bias ($\\beta_0$) term\n",
    "\n",
    "   <br>\n",
    "\n",
    "11. **What are some causes for under-fitting ? How to avoid them ? What disadvantages do they bring ?**\n",
    "\n",
    "   - **Causes:**\n",
    "     - Overly simplistic model, not enough feature\n",
    "     - Not enough polynomial / transformation of features\n",
    " \n",
    "   - **Consequences:**\n",
    "     - High bias, low variance\n",
    "     - Inaccurate predictions due to under-learning from the data\n",
    "   \n",
    "   - **Solution:**\n",
    "     - More features and complex (non-linear) model\n",
    "\n",
    "   <br>\n",
    "   \n",
    "12. **What are some causes for over-fitting ? How to avoid them ?**\n",
    "\n",
    "   - **Causes:**\n",
    "     - Overly complex model, too many feature\n",
    "     - Too many polynomial / transformation of features\n",
    " \n",
    "   - **Consequences:**\n",
    "     - Low bias, high variance\n",
    "     - Inaccurate predictions due to over-learning from noise and edge cases in the data\n",
    "   \n",
    "   - **Solution:**\n",
    "     - Regularization\n",
    "     - Feature selection\n",
    "     - Use K-fold cross validation to decide complexity of model\n",
    "     \n",
    "   <br>\n",
    "\n",
    "13. **How to spot an outlier in multiple linear regression ?**\n",
    "\n",
    "   - **Univariate scatter plots / Boxplots**\n",
    "   - **Normalized residual plot:**\n",
    "     -  More than 2 standard deviation from mean considered as outlier\n",
    "   - High residual values in additional to high leverage makes an influential outlier\n",
    "\n",
    "   <br>\n",
    "   \n",
    "14. **What does it mean in multiple regression when a point has high levarage ? What constitue an influential point ? Why are they important ?**\n",
    "\n",
    "   - **High levarage** means some feature(s) of a data point far deviates from the mean value of the feature(s) across data point\n",
    "   - **Influence = Leverage x Residual**\n",
    "   - High leverage and high residual data points are influential, i.e. affects the model ($\\beta$s) a lot when absent / present\n",
    "   - **Important because you do not a few data points controlling your model, and if so, you should know why**\n",
    "    \n",
    "    <br>\n",
    "\n",
    "15. **How to determine how many features to include for your model ? What difference does it make if you are fitting on on a large (does not fit into memory) dataset ?**\n",
    "    \n",
    "   - Forwards / Backwards elimination (Not feasible with big dataset, computationally intensive)\n",
    "   - Lasso (L1) regularization (shrink unimportant features towards zero)\n",
    "   - Random forest feature importance\n",
    "   - Adjusted $R^2$ with K-fold cross validation (F1 score with K-fold if logistic regression)\n",
    "   - Product knowledge\n",
    "\n",
    "    <br>\n",
    "  \n",
    "16. **What is the cost function for linear regression ?**\n",
    "\n",
    "   - **Mean Squared Error (MSE)**, $\\sum_{i=1}^n (\\hat{y} - y)^2$\n",
    "\n",
    "    <br>\n",
    "    \n",
    "17. **Using the cost function, describe two algorithms to fit a linear regression model.**\n",
    "  \n",
    "   **Analytical Approach**\n",
    "   - Take the partial derivate with respect to the particular $\\beta$ of the cost function\n",
    "     \n",
    "     $$\\frac{\\partial}{\\partial{\\beta_i}} \\sum_{d=1}^n (\\beta^T X - y)^2$$\n",
    "   \n",
    "   **Gradient Descent**\n",
    "   - If regularization / transformation / polynomial terms are involved (derivate not closed-form)\n",
    "   - \n",
    "\n",
    "   <br>\n",
    "  \n",
    "18. **What are the differences between running linear regression on a large (does not fit into memory) vs a small dataset ?**\n",
    "\n",
    "    <br>\n",
    "   \n",
    "19. **What is the difference between stochastic gradient methods and batched gradient methods ?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression\n",
    "\n",
    "**Questions for Linear Regression are applicable to Logistic Regression, except for the ones listed below.**  \n",
    "\n",
    "<br>\n",
    "\n",
    "1. **Interpretation of logistic regression model coefficients**\n",
    "\n",
    "   <br>\n",
    "   \n",
    "2. **How to find the decision boundary for a trained logistic regression model ? (Work through an example when regressed with 1 feature)**\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. **What is the cost function for logistic regression ?**\n",
    "\n",
    "   <br>\n",
    "   \n",
    "4. **Coefficient of features is the opposite sign of what you expect ? What would you do ?**\n",
    "\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
