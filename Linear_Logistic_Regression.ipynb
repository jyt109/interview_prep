{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Regression\n",
    "\n",
    "1. What is linear regression ?\n",
    "   - Algorithm that use a linear combination of features to predict a continuous target\n",
    "   - Weights are multiplied to each of the features prior to linear combination\n",
    "   - Weights are adjusted based on the error of the linear combination with the actual target value\n",
    "\n",
    "   <br>\n",
    "\n",
    "2. What are the assumptions of a linear regression model ?\n",
    "   - **Constant variance** of feature across different target values (homoscedasticity)\n",
    "   - **Normal distribution** of residuals of the model\n",
    "   - **Independence** between data points (lack of autocorrelation)\n",
    "   - **Linearity** of linear combination of features and target\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. What are the methods to test the assumptions ?\n",
    "   - **Heteroscedasticity** if residual plot presents a fanning pattern\n",
    "   - **Non-normal** if the quantiles are not aligned in the Q-Q plot with the normal quantiles\n",
    "   - **Dependence or Non-linearity** if the clear trends are observed in a sequence of points in residual plot\n",
    "\n",
    "   <br>\n",
    "   \n",
    "4. What are the steps one can take to correct for the violated assumptions ?\n",
    "   - **Heteroscedasticity** can be reduced by taking log of the features / target variables\n",
    "   - **Non-normality** of residuals can be reduced by taking log of the features / target variables\n",
    "   - **Dependence** can be taken into account by including lag terms or more complex time series model\n",
    "   - **Non-linearity** can be taken into account by including square or interaction term\n",
    "   \n",
    "   <br>\n",
    "   \n",
    "5. What is the interpretation of $\\beta$ ? What does $\\beta_0$ (the bias) mean ?\n",
    "\n",
    "   - A unit of increase of a certain feature is correlated with $\\beta$ amount of increase / decrease in the target value holding all the other features constant\n",
    "   - $\\beta_0$ represents the mean value of target when all the other features have a value of zero\n",
    "\n",
    "   <br>\n",
    "\n",
    "6. How does the interpretation of $\\beta$ change if log is taken on the the features / target variables ?\n",
    "   - **Logging the target:** \n",
    "   \n",
    "     A unit of increase of a feature correlated with (100 x $\\beta$) % increase / decrease in the target\n",
    "     \n",
    "     <br>\n",
    "     \n",
    "   - **Logging a feature:** \n",
    "   \n",
    "     1 % of increase of a feature correlated with ($\\beta$ / 100) units increase / decrease in the target\n",
    "   \n",
    "     <br>\n",
    "\n",
    "   - **Logging a feature and the target:** \n",
    "   \n",
    "     1 % of increase of a feature correlated with $\\beta$ % increase / decrease in the target\n",
    "   \n",
    "     <br>\n",
    "\n",
    "6. What is the interpretation of the p-value of $\\beta$ ? What is the null and alternative hypothesis ?\n",
    "\n",
    "   - **Null:** $\\beta =$ 0 \n",
    "   - **Alternative:** $\\beta \\neq$  0\n",
    "   - If the p-value is less than the significance level, then we can reject the null of $\\beta =$ 0, i.e. feature is not correlated to changes in the target variable\n",
    "\n",
    "   <br>\n",
    "\n",
    "7. Can you compare beta coefficient in multiple linear regression ? If not, what will allow us to ?\n",
    "\n",
    "   - Normalizing (substract by mean and divided by standard deviation column-wise) the features and target\n",
    "   - Since variables are normalized, we do not need to fit a bias ($\\beta_0$) term\n",
    "\n",
    "   <br>\n",
    "\n",
    "8. What are some causes for under-fitting ? How to avoid them ? What disadvantages do they bring ?\n",
    "\n",
    "   - **Causes:**\n",
    "     - Overly simplistic model, not enough feature\n",
    "     - Not enough polynomial / transformation of features\n",
    " \n",
    "   - **Consequences:**\n",
    "     - High bias, low variance\n",
    "     - Inaccurate predictions due to under-learning from the data\n",
    "   \n",
    "   - **Solution:**\n",
    "     - More features and complex (non-linear) model\n",
    "\n",
    "   <br>\n",
    "   \n",
    "9. What are some causes for over-fitting ? How to avoid them ? \n",
    "\n",
    "   - **Causes:**\n",
    "     - Overly complex model, too many feature\n",
    "     - Too many polynomial / transformation of features\n",
    " \n",
    "   - **Consequences:**\n",
    "     - Low bias, high variance\n",
    "     - Inaccurate predictions due to over-learning from noise and edge cases in the data\n",
    "   \n",
    "   - **Solution:**\n",
    "     - Regularization\n",
    "     - Feature selection\n",
    "     - Use K-fold cross validation to decide complexity of model\n",
    "     \n",
    "   <br>\n",
    "\n",
    "10. How to spot an outlier ? \n",
    "\n",
    "    - **Univariate scatter plots / Boxplots**\n",
    "    - **Normalized residual plot:**\n",
    "      -  More than 2 standard deviation from mean considered as outlier\n",
    "    - High residual values in additional to high leverage makes an influential outlier\n",
    "\n",
    "    <br>\n",
    "   \n",
    "11. What does it mean in multiple regression when a point has high levarage ? What constitue an influential point ? Why are they important ?\n",
    "\n",
    "    - **High levarage** means some feature(s) of a data point far deviates from the mean value of the feature(s) across data point\n",
    "    - **Influence = Leverage x Residual**\n",
    "    - High leverage and high residual data points are influential, i.e. affects the model ($\\beta$s) a lot when absent / present\n",
    "    - **Important because you do not a few data points controlling your model, and if so, you should know why**\n",
    "    \n",
    "    <br>\n",
    "\n",
    "12. How to determine how many features to include for your model ? What difference does it make if you are fitting on on a large (does not fit into memory) dataset ?  \n",
    "    \n",
    "    - \n",
    "\n",
    "    <br>\n",
    "  \n",
    "13. What is the cost function for linear regression ? \n",
    "\n",
    "    <br>\n",
    "    \n",
    "14. Using the cost function, describe an algorithm to fit a linear regression model.\n",
    "\n",
    "    <br>\n",
    "  \n",
    "15. What are the differences between running linear regression on a large (does not fit into memory) vs a small dataset ?\n",
    "\n",
    "    <br>\n",
    "   \n",
    "16. What is the difference between stochastic gradient methods and batched gradient methods ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression\n",
    "\n",
    "1. How to prevent over-fitting ?\n",
    "\n",
    "2. How to validate model\n",
    "\n",
    "3. Cost function\n",
    "\n",
    "4. Algorithm for logit\n",
    "\n",
    "5. Difference between AIC and BIC\n",
    "\n",
    "6. ROC curve\n",
    "\n",
    "7. Difference between logistic and random forest\n",
    "\n",
    "8. Decision boundary\n",
    "\n",
    "wrong sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
