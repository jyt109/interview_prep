{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Decision Tree\n",
    "\n",
    "1. **What is entropy ?**\n",
    "   \n",
    "   - How uncertain you are \n",
    "   - More uncertain about what color marble picked if half of the marbles are red and half are blue\n",
    "   - No uncertainty about what color marble picked if all marbles are red\n",
    "   - Shannon entropy: $- p \\cdot log_2(p)$\n",
    "   \n",
    "   <br>\n",
    "\n",
    "2. **How does decision tree work ? (For classification)**\n",
    "\n",
    "   - Decide which feature to split on that will give the least entropy overall\n",
    "   - Sum of entropies of 2 groups after split\n",
    "   - The smaller the better\n",
    "   - Repeat the process\n",
    "   - Stop if: \n",
    "     - No features left\n",
    "     - No samples left\n",
    "\n",
    "   <br>\n",
    "   \n",
    "3. **How to avoid overfitting (low bias and high variance) ?**\n",
    "\n",
    "   - Limit max depth of tree \n",
    "   - Reduce depth of tree after fitting (Post pruning)\n",
    "  \n",
    "   <br>\n",
    "   \n",
    "3. **Why is post-pruning better than max depth for avoiding over-fitting ?**\n",
    "\n",
    "   - Post-pruning merges pair of nodes from the lowest level of tree and compute test error\n",
    "   - Stop until the test error does not decrease anymore\n",
    "   - Post-pruning gives the tree depth that provides the best prediction\n",
    "   - Max depth does not promise that, but max depth is less computationally intensive\n",
    "  \n",
    "   <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest \n",
    "\n",
    "1. **Why is random forest perform better than decision tree ?**\n",
    "\n",
    "   - More decision trees\n",
    "   - **Reduce variance while not increasing bias**\n",
    "     - **_Boostrapping (sample with replacement, same size) training data for each tree_**\n",
    "     - **_Sample of n features for each tree_** \n",
    "   - Low bias since all trees are fully grown to the bottom\n",
    "   \n",
    "   <br>\n",
    "\n",
    "2. **Why sample data and features for each tree ?**\n",
    "   \n",
    "   - Reduce variance\n",
    "   - Reduce correlation between trees\n",
    "   - If trees are correlated, effectively fewer trees built\n",
    "   - Ensemble effect of tree is less and more variance\n",
    "   - Sampling of features allows less predictive features to be considered, without domination of the most predictive features\n",
    "   - Reduce bias and boost performance in particular special cases\n",
    "\n",
    "   <br>\n",
    "   \n",
    "3. **What is out-of-bag (OOB) error ?**\n",
    "\n",
    "   - The data that is not selected for each bootstrap sample for each tree is out-of-bag\n",
    "   - Each out-of-bag data point is run through fitted trees that **did not use that data point**\n",
    "   - The prediction for the out-of-bag point is the majority class in the predictions above\n",
    "   - Which is compared to the prediction running through all the trees (majority)\n",
    "\n",
    "   <br>\n",
    "   \n",
    "4. **What is the proximity matrix and how is it computed ?**\n",
    "   \n",
    "   - Proximity matrix is a `N * N` matrix describing how close are each of the data point with each other \n",
    "   - For each tree, run the the data point down the tree and record the terminal node\n",
    "   - If 2 nodes ended up in the same terminal node, add one to their proximity\n",
    "   - Supervised clustering\n",
    "\n",
    "   <br>\n",
    "   \n",
    "5. **How do you use the proximity matrix to imputate missing values ?**\n",
    "\n",
    "   - **Use proximity matrix**\n",
    "   - First imputate missing values with median of the feature\n",
    "   - Fit the trees and compute proximity matrix\n",
    "   - Imputate each missing value by taking a weighted average (by proximity) of the feature that is missing\n",
    "   - Fit the tree again, compute proximity matrix and repeat the proces\n",
    "\n",
    "   <br>\n",
    "   \n",
    "6. **How is feature importance computed ?**\n",
    "\n",
    "   - Generate predictions for OOB data by running down trees that don't include the paritcular OOB sample\n",
    "   - The prediction is generated by taking the marjority class amongst the trees \n",
    "   - Randomize one feature across all rows of OOB with others features untouched (randomized OOB)\n",
    "   - Genearte predictions for the randomized OOB similarly\n",
    "   - `Original OOB Correct Prediction - Randomized OOB Correct Prediction`\n",
    "\n",
    "   <br>\n",
    "   \n",
    "6. **What is random forest better/worse than logistic regression at ?**\n",
    "\n",
    "   - RF is non-linear\n",
    "   - Fits features with that have non-linear relationship with target better\n",
    "   - Better at fitting wide datasets (simply run out of sample and stop growing tree on less important features)\n",
    "   - Variable importance only tells you if the feature contributes to better prediction\n",
    "   - Beta coefficients for logistic regression are more informative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
